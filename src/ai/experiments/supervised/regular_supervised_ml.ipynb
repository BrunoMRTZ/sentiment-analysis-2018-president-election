{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:09:24.747964Z",
     "start_time": "2018-11-15T21:09:19.816530Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import copy\n",
    "import re\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from utils import tokenizer, load_six_emotions, load_3_emotions\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.warnings.filterwarnings('ignore')\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Fornecido pela equipe da PUC-PR através do site [EMOÇÕES.BR](http://www.ppgia.pucpr.br/~paraiso/mineracaodeemocoes/index.php), contém cerca de 1000 frases, categorizadas usando as 6 emoções de Ekman (alegria, surpresa, tristeza, medo, raiva, desgosto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:09:29.579843Z",
     "start_time": "2018-11-15T21:09:24.750576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ALEGRIA', 'gabriel destac rapidez extraçã óle tup barril entreg rio janeir president lul gabriel diss lágrim extraçã primeir óle tup'), ('DESGOSTO', 'estiag provoc queim regiã central país bombeir precis cont dois incêndi goiân julh dest ano registr foc'), ('DESGOSTO', 'rapaz pres cnhs fals camp grand segund políc vend document dit políc aprend faz falsific web'), ('ALEGRIA', 'senador piau recup bem após cirurg reduçã estômag heráclit fort dem caminh corredor hospital assessor diz senador dev fic licenc seman'), ('DESGOSTO', 'substitut faust sanct lib vend gad oportunity antes sair fér juiz cas determin sequestr gad apont lavag dinheir grup daniel dant fazend')]\n"
     ]
    }
   ],
   "source": [
    "def carregar(filename):\n",
    "    frases = []\n",
    "    with open(filename, 'r') as h:\n",
    "        reader = csv.reader(h, delimiter='|')\n",
    "        for row in reader:\n",
    "            frase = tokenizer(row[0]).strip()\n",
    "            sentimento = row[1].upper()\n",
    "            if len(frase) > 5:\n",
    "                frases.append((sentimento, frase))\n",
    "    return frases\n",
    "\n",
    "frases = carregar('/home/rdenadai/vagrant/python-dev/sentiment-analysis-2018-president-election/dataset/puc-pr/g1_v1.csv')\n",
    "\n",
    "shuffle(frases)\n",
    "\n",
    "print(frases[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:09:29.593122Z",
     "start_time": "2018-11-15T21:09:29.582014Z"
    }
   },
   "outputs": [],
   "source": [
    "afrases = []\n",
    "asentimentos =[]\n",
    "for sentimento, frase in frases:\n",
    "    afrases.append(frase)\n",
    "    asentimentos.append(sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:09:29.718523Z",
     "start_time": "2018-11-15T21:09:29.596057Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_ml_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = np.round(model.score(X_test, y_test) * 100, 2)\n",
    "    print(f'Modelo   : {model.__class__.__name__}')\n",
    "    print(f'Acurácia : {accuracy}%')\n",
    "    print('-' * 20)\n",
    "    return accuracy\n",
    "\n",
    "def split_data(X, y):\n",
    "    test_size = .33\n",
    "    random_state = 0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:09:29.874603Z",
     "start_time": "2018-11-15T21:09:29.720817Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = (\n",
    "    MultinomialNB(),\n",
    "    ComplementNB(),\n",
    "    LogisticRegression(multi_class='auto', solver='lbfgs'),\n",
    "    RandomForestClassifier(n_estimators=50, min_samples_split=5, random_state=0),\n",
    "    KNeighborsClassifier(n_neighbors=8, algorithm='auto'),\n",
    "    MLPClassifier(hidden_layer_sizes=(250,), max_iter=1000),\n",
    "    LinearSVC(max_iter=1000),\n",
    "    SVC(gamma='auto', max_iter=1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:09:30.130270Z",
     "start_time": "2018-11-15T21:09:29.876962Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_tfidf = vec_tfidf.fit_transform(afrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:10:58.016018Z",
     "start_time": "2018-11-15T21:09:30.132479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 45.92%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 45.02%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 45.62%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 47.13%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 43.5%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 48.04%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 50.76%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 37.76%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_tfidf, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA (usando TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:11:06.519591Z",
     "start_time": "2018-11-15T21:10:58.019358Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 38.37%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 41.99%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 45.32%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 46.83%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 41.69%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 47.43%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 43.81%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 37.76%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X_svd = lsa.fit_transform(X_tfidf)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, asentimentos))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (usando TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:11:41.977840Z",
     "start_time": "2018-11-15T21:11:06.522248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 32.02%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 22.05%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 32.02%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 31.42%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 31.42%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 29.91%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 32.33%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 37.76%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=100, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lda = make_pipeline(lda, normalizer)\n",
    "X_lda = lda.fit_transform(X_tfidf)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:11:42.125379Z",
     "start_time": "2018-11-15T21:11:41.985529Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_count = CountVectorizer(ngram_range=(1, 2))\n",
    "X_count = vec_count.fit_transform(afrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:12:28.562206Z",
     "start_time": "2018-11-15T21:11:42.127726Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 47.43%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 35.95%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 51.36%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 46.83%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 37.46%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 48.94%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 50.76%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 37.76%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_count, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA (usando Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:12:37.808454Z",
     "start_time": "2018-11-15T21:12:28.564813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 37.76%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 38.97%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 46.83%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 43.5%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 41.39%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 44.71%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 44.71%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 37.76%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lda = make_pipeline(svd, normalizer)\n",
    "X_svd = lda.fit_transform(X_count)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (usando Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:12:48.555053Z",
     "start_time": "2018-11-15T21:12:37.811253Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 29.31%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 25.68%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 29.31%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 30.51%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 29.31%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 29.61%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 29.31%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 37.76%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=100, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lda = make_pipeline(lda, normalizer)\n",
    "X_lda = lda.fit_transform(X_count)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count + TF-IDF + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:17:01.814788Z",
     "start_time": "2018-11-15T21:12:48.557108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17906692, 18842000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count\n",
    "vec_count = CountVectorizer()\n",
    "X_count = vec_count.fit_transform(afrases)\n",
    "weights_count = pd.DataFrame(np.round(X_count.toarray().T, 8), index=vec_count.get_feature_names())\n",
    "\n",
    "# TF-IDF\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vec_tfidf.fit_transform(afrases)\n",
    "weights_tfidf = pd.DataFrame(np.round(X_tfidf.toarray().T, 8), index=vec_tfidf.get_feature_names())\n",
    "\n",
    "# Word2Vec preprocessing\n",
    "frases_w2v = []\n",
    "for frase in afrases:\n",
    "    bigram = []\n",
    "    p_frase = word_tokenize(frase)\n",
    "    for m, palavra in enumerate(p_frase):\n",
    "        next_p = None\n",
    "        try:\n",
    "            next_p = p_frase[m+1]\n",
    "        except:\n",
    "            pass\n",
    "        bigram += [f'{palavra}']\n",
    "#         if next_p:\n",
    "#             bigram += [f'{palavra} {next_p}']\n",
    "    frases_w2v += [bigram]\n",
    "\n",
    "# Word2Vec\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=frases_w2v,\n",
    "    sg=1,\n",
    "    hs=1,\n",
    "    size=1,\n",
    "    window=25,\n",
    "    min_count=1,\n",
    "    seed=0,\n",
    "    workers=10)\n",
    "model.train(frases_w2v, total_examples=len(frases_w2v), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:17:03.204055Z",
     "start_time": "2018-11-15T21:17:01.816639Z"
    }
   },
   "outputs": [],
   "source": [
    "r_words = {}\n",
    "for word in vec_count.get_feature_names():\n",
    "    idx = weights_count.index.get_loc(word)\n",
    "    w2c_val = .1\n",
    "    try:\n",
    "        w2c_val = model.wv[word]\n",
    "    except:\n",
    "        pass\n",
    "    r_words[word] = (weights_tfidf.iloc[idx].values + weights_count.iloc[idx].values) * w2c_val\n",
    "lwor = list(r_words.keys())\n",
    "X = np.asarray(list(r_words.values()))\n",
    "weights = pd.DataFrame(X, index=lwor)\n",
    "X = X.T\n",
    "\n",
    "normalizer = Normalizer(copy=False)\n",
    "X = normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T21:17:59.945366Z",
     "start_time": "2018-11-15T21:17:03.207815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 52.27%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 46.22%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 50.76%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 47.13%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 42.9%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 45.92%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 49.55%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 37.76%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "31px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
