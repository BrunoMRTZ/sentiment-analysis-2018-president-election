{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T13:06:25.460061Z",
     "start_time": "2018-11-15T13:06:22.095243Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import copy\n",
    "import re\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from utils import tokenizer, load_six_emotions, load_3_emotions\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.warnings.filterwarnings('ignore')\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Fornecido pela equipe da PUC-PR através do site [EMOÇÕES.BR](http://www.ppgia.pucpr.br/~paraiso/mineracaodeemocoes/index.php), contém cerca de 1000 frases, categorizadas usando as 6 emoções de Ekman (alegria, surpresa, tristeza, medo, raiva, desgosto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:02:21.199559Z",
     "start_time": "2018-11-15T19:02:18.699893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ALEGRIA', 'fri nã afast alun piscin curitib piscin aquec receb pesso dia segund prefeitur professor diz alun pass adapt iníci ativ'), ('ALEGRIA', 'eua alenc obtém autoriz pass nov tratament contr cânc vic president receb nov medic tratament atac célul provoc tumor'), ('DESGOSTO', 'milit faz barric torn cas líd nigér extrem islâm autodenomin talibã deix mais cem mort ataqu contr civ polic fim seman'), ('DESGOSTO', 'funcionári acus govern par favorec madeireir esquem pod ter moviment milhõ secretári mei ambient anunc saíd carg'), ('DESGOSTO', 'quatr palestin fic fer bombardei israelens tún gaz ataqu respost lançament foguet palestin contr israel cinc tún abastec faix gaz sid destruíd')]\n"
     ]
    }
   ],
   "source": [
    "def carregar(filename):\n",
    "    frases = []\n",
    "    with open(filename, 'r') as h:\n",
    "        reader = csv.reader(h, delimiter='|')\n",
    "        for row in reader:\n",
    "            frase = tokenizer(row[0]).strip()\n",
    "            sentimento = row[1].upper()\n",
    "            if len(frase) > 5:\n",
    "                frases.append((sentimento, frase))\n",
    "    return frases\n",
    "\n",
    "frases = carregar('/home/rdenadai/vagrant/python-dev/sentiment-analysis-2018-president-election/dataset/puc-pr/g1_v1.csv')\n",
    "\n",
    "shuffle(frases)\n",
    "\n",
    "print(frases[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:02:22.729143Z",
     "start_time": "2018-11-15T19:02:22.723439Z"
    }
   },
   "outputs": [],
   "source": [
    "afrases = []\n",
    "asentimentos =[]\n",
    "for sentimento, frase in frases:\n",
    "    afrases.append(frase)\n",
    "    asentimentos.append(sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:02:24.642194Z",
     "start_time": "2018-11-15T19:02:24.620841Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_ml_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = np.round(model.score(X_test, y_test) * 100, 2)\n",
    "    print(f'Modelo   : {model.__class__.__name__}')\n",
    "    print(f'Acurácia : {accuracy}%')\n",
    "    print('-' * 20)\n",
    "    return accuracy\n",
    "\n",
    "def split_data(X, y):\n",
    "    test_size = .33\n",
    "    random_state = 0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:02:27.784274Z",
     "start_time": "2018-11-15T19:02:27.778063Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = (\n",
    "    MultinomialNB(),\n",
    "    ComplementNB(),\n",
    "    LogisticRegression(multi_class='auto', solver='lbfgs'),\n",
    "    RandomForestClassifier(n_estimators=50, min_samples_split=5, random_state=0),\n",
    "    KNeighborsClassifier(n_neighbors=8, algorithm='auto'),\n",
    "    MLPClassifier(hidden_layer_sizes=(250,), max_iter=1000),\n",
    "    LinearSVC(max_iter=1000),\n",
    "    SVC(gamma='auto', max_iter=1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:02:29.050339Z",
     "start_time": "2018-11-15T19:02:28.948512Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_tfidf = vec_tfidf.fit_transform(afrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:03:45.227871Z",
     "start_time": "2018-11-15T19:02:29.677653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 51.36%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 45.92%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 50.15%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 50.15%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 48.94%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 51.06%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 53.17%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 39.27%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_tfidf, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA (usando TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:03:59.146140Z",
     "start_time": "2018-11-15T19:03:45.231669Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 39.58%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 47.43%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 52.57%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 50.15%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 43.5%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 49.55%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 52.27%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 39.27%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X_svd = lsa.fit_transform(X_tfidf)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, asentimentos))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (usando TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:04:31.235676Z",
     "start_time": "2018-11-15T19:03:59.149905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 35.65%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 27.49%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 36.25%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 36.25%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 36.56%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 36.56%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 34.74%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 39.27%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=100, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lda = make_pipeline(lda, normalizer)\n",
    "X_lda = lda.fit_transform(X_tfidf)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:04:31.366344Z",
     "start_time": "2018-11-15T19:04:31.241323Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_count = CountVectorizer(ngram_range=(1, 2))\n",
    "X_count = vec_count.fit_transform(afrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:05:14.184202Z",
     "start_time": "2018-11-15T19:04:31.368862Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 48.64%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 34.14%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 54.08%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 49.24%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 37.76%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 51.96%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 53.78%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 39.27%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_count, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA (usando Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:05:37.873233Z",
     "start_time": "2018-11-15T19:05:14.186755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 39.27%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 45.32%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 49.85%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 48.04%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 40.18%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 48.04%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 47.43%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 39.27%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lda = make_pipeline(svd, normalizer)\n",
    "X_svd = lda.fit_transform(X_count)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (usando Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:05:54.355398Z",
     "start_time": "2018-11-15T19:05:37.875561Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 36.56%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 29.0%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 36.86%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 35.05%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 41.39%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 35.95%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 36.56%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 39.27%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=100, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lda = make_pipeline(lda, normalizer)\n",
    "X_lda = lda.fit_transform(X_count)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count + TF-IDF + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:09:08.897903Z",
     "start_time": "2018-11-15T19:05:54.357732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15706965, 18842000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count\n",
    "vec_count = CountVectorizer()\n",
    "X_count = vec_count.fit_transform(afrases)\n",
    "weights_count = pd.DataFrame(np.round(X_count.toarray().T, 8), index=vec_count.get_feature_names())\n",
    "\n",
    "# TF-IDF\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vec_tfidf.fit_transform(afrases)\n",
    "weights_tfidf = pd.DataFrame(np.round(X_tfidf.toarray().T, 8), index=vec_tfidf.get_feature_names())\n",
    "\n",
    "# Word2Vec preprocessing\n",
    "frases_w2v = []\n",
    "for frase in afrases:\n",
    "    bigram = []\n",
    "    p_frase = word_tokenize(frase)\n",
    "    for m, palavra in enumerate(p_frase):\n",
    "        next_p = None\n",
    "        try:\n",
    "            next_p = p_frase[m+1]\n",
    "        except:\n",
    "            pass\n",
    "        bigram += [f'{palavra}']\n",
    "#         if next_p:\n",
    "#             bigram += [f'{palavra} {next_p}']\n",
    "    frases_w2v += [bigram]\n",
    "\n",
    "# Word2Vec\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=frases_w2v,\n",
    "    sg=1,\n",
    "    hs=1,\n",
    "    size=1,\n",
    "    window=15,\n",
    "    min_count=2,\n",
    "    seed=0,\n",
    "    workers=10)\n",
    "model.train(frases_w2v, total_examples=len(frases_w2v), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:09:10.262003Z",
     "start_time": "2018-11-15T19:09:08.899770Z"
    }
   },
   "outputs": [],
   "source": [
    "r_words = {}\n",
    "for word in vec_count.get_feature_names():\n",
    "    idx = weights_count.index.get_loc(word)\n",
    "    try:\n",
    "        r_words[word] = (weights_tfidf.iloc[idx].values + weights_count.iloc[idx].values) * model.wv[word]\n",
    "    except:\n",
    "        pass\n",
    "lwor = list(r_words.keys())\n",
    "X = np.asarray(list(r_words.values()))\n",
    "weights = pd.DataFrame(X, index=lwor)\n",
    "X = X.T\n",
    "\n",
    "normalizer = Normalizer(copy=False)\n",
    "X = normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T19:09:46.631774Z",
     "start_time": "2018-11-15T19:09:10.267983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo   : MultinomialNB\n",
      "Acurácia : 53.47%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 47.73%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 54.68%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 51.36%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 49.85%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 48.94%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 53.17%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 39.27%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X, asentimentos))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "31px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
