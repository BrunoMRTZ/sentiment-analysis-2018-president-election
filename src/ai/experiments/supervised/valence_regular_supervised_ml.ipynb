{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAyZCtgcISFO"
   },
   "source": [
    "## Testes de valência para o projeto final de IA369Y 2 Semestre 2018\n",
    "\n",
    "Passos para tratar os dados com valência, testar e escolher um classificador para utilizar no projeto final de IA369Y.\n",
    "\n",
    "1) Remover espaços duplos, quebras de linha, números e links do dataset e das frases a serem testadas.\n",
    "\n",
    "2) Remover stopwords e aplicar o stemmer.\n",
    "\n",
    "3) Treinar os classificadores.\n",
    "\n",
    "4) Realizar as predições com os classificadores.\n",
    "\n",
    "5) Avaliar as medidas obtidas com os classificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T18:57:29.903136Z",
     "start_time": "2018-11-22T18:57:15.412723Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "r2Wwyp_VHu32",
    "outputId": "3aef9901-ce0a-44de-df61-b6eeb50e676b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import copy\n",
    "import re\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from utils import tokenizer, load_six_emotions, load_3_emotions\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.warnings.filterwarnings('ignore')\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thdVOHWmKxaF"
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Para validar, serão utilizados dois datasets.\n",
    "\n",
    "Os dos datasets foram obtidos do site minerando dados.\n",
    "\n",
    "O primeiro deles tem tweets de política de Minas Gerais com rótulos de valência: positivo, negativo e neutro. Foi feito um tratamento para eliminar tweets repetidos e dessa forma sobraram 3016 tweets.\n",
    "\n",
    "O segundo contém 2123 títulos de notícias com rótulos de valência: positivo, negativo e neutro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T18:57:39.797742Z",
     "start_time": "2018-11-22T18:57:37.836160Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "colab_type": "code",
    "id": "r600QV9JQz4z",
    "outputId": "74e465fe-dfac-40a6-8904-3992124e3b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1152\n",
      "drwxr-xr-x 3 rdenadai rdenadai    4096 Nov 22 16:52 .\n",
      "drwxr-xr-x 5 rdenadai rdenadai    4096 Nov 22 15:59 ..\n",
      "-rw-r--r-- 1 rdenadai rdenadai   21708 Nov 22 16:52 emotion_regular_supervised_ml.ipynb\n",
      "drwxr-xr-x 2 rdenadai rdenadai    4096 Nov 22 16:06 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 rdenadai rdenadai 1142042 Nov 22 16:05 valence_regular_supervised_ml.ipynb\n",
      "--2018-11-22 16:57:38--  https://raw.githubusercontent.com/rdenadai/sentiment-analysis-2018-president-election/edgarbanhesse/material-apoio/tweets_mg_tratados.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.92.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.92.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 295061 (288K) [text/plain]\n",
      "Saving to: ‘tweets_mg_tratados.csv’\n",
      "\n",
      "tweets_mg_tratados. 100%[===================>] 288.15K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2018-11-22 16:57:38 (32.6 MB/s) - ‘tweets_mg_tratados.csv’ saved [295061/295061]\n",
      "\n",
      "--2018-11-22 16:57:39--  https://raw.githubusercontent.com/rdenadai/sentiment-analysis-2018-president-election/edgarbanhesse/material-apoio/titulo_noticias.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.92.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.92.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152016 (148K) [text/plain]\n",
      "Saving to: ‘titulo_noticias.txt’\n",
      "\n",
      "titulo_noticias.txt 100%[===================>] 148.45K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2018-11-22 16:57:39 (9.25 MB/s) - ‘titulo_noticias.txt’ saved [152016/152016]\n",
      "\n",
      "--2018-11-22 16:57:39--  https://raw.githubusercontent.com/rdenadai/sentiment-analysis-2018-president-election/edgarbanhesse/material-apoio/50_tweets_mg.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.92.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.92.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4912 (4.8K) [text/plain]\n",
      "Saving to: ‘50_tweets_mg.csv’\n",
      "\n",
      "50_tweets_mg.csv    100%[===================>]   4.80K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-11-22 16:57:39 (120 MB/s) - ‘50_tweets_mg.csv’ saved [4912/4912]\n",
      "\n",
      "total 1604\n",
      "drwxr-xr-x 3 rdenadai rdenadai    4096 Nov 22 16:57 .\n",
      "drwxr-xr-x 5 rdenadai rdenadai    4096 Nov 22 15:59 ..\n",
      "-rw-r--r-- 1 rdenadai rdenadai    4912 Nov 22 16:57 50_tweets_mg.csv\n",
      "-rw-r--r-- 1 rdenadai rdenadai   21708 Nov 22 16:52 emotion_regular_supervised_ml.ipynb\n",
      "drwxr-xr-x 2 rdenadai rdenadai    4096 Nov 22 16:06 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 rdenadai rdenadai  152016 Nov 22 16:57 titulo_noticias.txt\n",
      "-rw-r--r-- 1 rdenadai rdenadai  295061 Nov 22 16:57 tweets_mg_tratados.csv\n",
      "-rw-r--r-- 1 rdenadai rdenadai 1142042 Nov 22 16:05 valence_regular_supervised_ml.ipynb\n"
     ]
    }
   ],
   "source": [
    "#Download dos datasets\n",
    "!ls -la\n",
    "!rm -f *.csv\n",
    "!rm -f *.txt\n",
    "!wget https://raw.githubusercontent.com/rdenadai/sentiment-analysis-2018-president-election/edgarbanhesse/material-apoio/tweets_mg_tratados.csv\n",
    "!wget https://raw.githubusercontent.com/rdenadai/sentiment-analysis-2018-president-election/edgarbanhesse/material-apoio/titulo_noticias.txt\n",
    "!wget https://raw.githubusercontent.com/rdenadai/sentiment-analysis-2018-president-election/edgarbanhesse/material-apoio/50_tweets_mg.csv\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T18:57:48.997399Z",
     "start_time": "2018-11-22T18:57:43.475461Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "tJiBeMzSTKlB",
    "outputId": "56408718-b65c-42be-dbc7-703dc96ea551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NEUTRO', 'aéci govern min fez mais viagens pra rio janeir cad indign'), ('NEUTRO', 'concórd indic pap invest nest mês conf'), ('NEGATIVO', 'bat caminhã carr deix cinc mort sul min ger estad min'), ('NEGATIVO', 'obras evit rodízi águ paul som milhõ'), ('POSITIVO', 'filhob menor aprend roub tráfic drog uberlând')]\n"
     ]
    }
   ],
   "source": [
    "#Carregando os datasets\n",
    "def carregar(filename):\n",
    "    frases = []\n",
    "    with open(filename, 'r', encoding='utf-8') as h:\n",
    "        reader = csv.reader(h, delimiter='|')\n",
    "        for row in reader:\n",
    "            frase = tokenizer(row[0]).strip()\n",
    "            valencia = row[1].upper()\n",
    "            if len(frase) > 5:\n",
    "                frases.append((valencia, frase))\n",
    "    return frases\n",
    "\n",
    "frases = carregar('tweets_mg_tratados.csv')\n",
    "frases += carregar('titulo_noticias.txt')\n",
    "\n",
    "shuffle(frases)\n",
    "\n",
    "print(frases[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T18:58:20.953711Z",
     "start_time": "2018-11-22T18:58:15.659059Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "zD3gV2f0tsNL",
    "outputId": "5560031f-54b1-44a9-81e7-9e17f3f5417c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NEUTRO', 'bom band mort'), ('NEUTRO', 'fóruns region govern vã eleg nov prefeit vereador'), ('NEGATIVO', 'govern min ger compr mais dois helicópter'), ('POSITIVO', 'polic milit faz prisõ aprend armas fog drog bail funk'), ('POSITIVO', 'cab políc milit anos folg imped roub pad noit dest')]\n",
      "--------------------\n",
      "[('POLARIDADE', '\\ufeffnotic'), ('POSITIVO', 'diretor petrobr neg organiz crimin estatal notíc brasil'), ('NEUTRO', 'tom cautel janet yelen pression bols fortalec dól'), ('POSITIVO', 'bovesp caminh nov máxim ano quart alta segu'), ('NEGATIVO', 'após abrir estável ibovesp pass registr qued petrobr val')]\n"
     ]
    }
   ],
   "source": [
    "#Carrega os datasets em separado\n",
    "tweets_mg = []\n",
    "titulo_noticias = []\n",
    "\n",
    "tweets_mg = carregar('tweets_mg_tratados.csv')\n",
    "titulo_noticias = carregar('titulo_noticias.txt')\n",
    "\n",
    "print(tweets_mg[:5])\n",
    "print('-' * 20)\n",
    "print(titulo_noticias[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T18:58:55.332706Z",
     "start_time": "2018-11-22T18:58:55.324047Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "yJbG_ck5i8rI",
    "outputId": "8d2f6682-e474-43bc-8da5-2c1a6db52619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aéci govern min fez mais viagens pra rio janeir cad indign', 'concórd indic pap invest nest mês conf', 'bat caminhã carr deix cinc mort sul min ger estad min', 'obras evit rodízi águ paul som milhõ', 'filhob menor aprend roub tráfic drog uberlând']\n",
      "['NEUTRO', 'NEUTRO', 'NEGATIVO', 'NEGATIVO', 'POSITIVO']\n",
      "--------------------\n",
      "['bom band mort', 'fóruns region govern vã eleg nov prefeit vereador', 'govern min ger compr mais dois helicópter', 'polic milit faz prisõ aprend armas fog drog bail funk', 'cab políc milit anos folg imped roub pad noit dest']\n",
      "['NEUTRO', 'NEUTRO', 'NEGATIVO', 'POSITIVO', 'POSITIVO']\n",
      "--------------------\n",
      "['\\ufeffnotic', 'diretor petrobr neg organiz crimin estatal notíc brasil', 'tom cautel janet yelen pression bols fortalec dól', 'bovesp caminh nov máxim ano quart alta segu', 'após abrir estável ibovesp pass registr qued petrobr val']\n",
      "['POLARIDADE', 'POSITIVO', 'NEUTRO', 'POSITIVO', 'NEGATIVO']\n"
     ]
    }
   ],
   "source": [
    "#all_datasets\n",
    "afrases = []\n",
    "avalencias = []\n",
    "for valencia, frase in frases:\n",
    "    afrases.append(frase)\n",
    "    avalencias.append(valencia)\n",
    "    \n",
    "print(afrases[:5])\n",
    "print(avalencias[:5])\n",
    "print('-' * 20)\n",
    "\n",
    "\n",
    "#tweets_mg\n",
    "atweets_mg = []\n",
    "aval_tweets_mg = []\n",
    "for valencia, frase in tweets_mg:\n",
    "    atweets_mg.append(frase)\n",
    "    aval_tweets_mg.append(valencia)\n",
    "\n",
    "print(atweets_mg[:5])\n",
    "print(aval_tweets_mg[:5])\n",
    "print('-' * 20)\n",
    "\n",
    "#titulo_noticias\n",
    "atitulo_noticias = []\n",
    "aval_titulo_noticias = []\n",
    "for valencia, frase in titulo_noticias:\n",
    "    atitulo_noticias.append(frase)\n",
    "    aval_titulo_noticias.append(valencia)\n",
    "\n",
    "print(atitulo_noticias[:5])\n",
    "print(aval_titulo_noticias[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T18:58:58.664051Z",
     "start_time": "2018-11-22T18:58:58.660593Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UicV1eOVjX2Q"
   },
   "outputs": [],
   "source": [
    "def run_ml_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f'Modelo   : {model.__class__.__name__}')\n",
    "    print(f'Acurácia : {np.round(model.score(X_test, y_test) * 100, 2)}%')\n",
    "    print('-' * 20)\n",
    "\n",
    "def split_data(X, y):\n",
    "    test_size = .3\n",
    "    random_state = 0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k94TjXIfjjF6"
   },
   "source": [
    "## Classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T18:59:12.608064Z",
     "start_time": "2018-11-22T18:59:12.604759Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "D-xa0I3PjexY"
   },
   "outputs": [],
   "source": [
    "classifiers = (\n",
    "    MultinomialNB(),\n",
    "    ComplementNB(),\n",
    "    LogisticRegression(multi_class='auto', solver='lbfgs'),\n",
    "    RandomForestClassifier(n_estimators=50, min_samples_split=5, random_state=0),\n",
    "    KNeighborsClassifier(n_neighbors=8, algorithm='auto'),\n",
    "    MLPClassifier(hidden_layer_sizes=(100, 25), max_iter=500, random_state=0),\n",
    "    LinearSVC(max_iter=500),\n",
    "    SVC(gamma='auto', max_iter=500),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T0sft8Ujwki"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T18:59:14.508801Z",
     "start_time": "2018-11-22T18:59:14.270637Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "g9TiaAoGjvPj"
   },
   "outputs": [],
   "source": [
    "vec_tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_tfidf = vec_tfidf.fit_transform(afrases)\n",
    "\n",
    "vec_tfidf_tmg = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_tfidf_tmg = vec_tfidf_tmg.fit_transform(atweets_mg)\n",
    "\n",
    "vec_tfidf_tn = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_tfidf_tn = vec_tfidf_tn.fit_transform(atitulo_noticias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T19:03:23.986878Z",
     "start_time": "2018-11-22T18:59:17.580260Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "id": "1keF94erj8UT",
    "outputId": "2c5c1870-cf5f-4f72-8f2d-c0b48bf05bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_datasets\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 60.87%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 62.17%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 66.47%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 64.39%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 61.52%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 64.32%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 66.86%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 58.46%\n",
      "--------------------\n",
      "\n",
      "tweets_mg\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 62.18%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 61.07%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 65.18%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 63.74%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 61.07%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 62.74%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 64.29%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 61.07%\n",
      "--------------------\n",
      "\n",
      "titulo_noticias\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 61.85%\n",
      "--------------------\n",
      "Modelo   : ComplementNB\n",
      "Acurácia : 63.11%\n",
      "--------------------\n",
      "Modelo   : LogisticRegression\n",
      "Acurácia : 66.25%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 61.7%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 60.6%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 65.46%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 67.5%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 55.26%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nall_datasets\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_tfidf, avalencias))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\ntweets_mg\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_tfidf_tmg, aval_tweets_mg))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\ntitulo_noticias\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_tfidf_tn, aval_titulo_noticias))\n",
    "    except:\n",
    "        pass      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ny241RlukicC"
   },
   "source": [
    "## LSA (usando TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1054
    },
    "colab_type": "code",
    "id": "C2B7FluMklZB",
    "outputId": "b867c791-ef17-4df6-9f27-6fb0870dacd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_datasets\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 42.13%\n",
      "--------------------\n",
      "multi_class should be either multinomial or ovr, got auto\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 59.88%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 55.27%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 57.22%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 61.44%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 42.78%\n",
      "--------------------\n",
      "\n",
      "tweets_mg\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 45.17%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 62.15%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 58.27%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 62.71%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 63.82%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 44.95%\n",
      "--------------------\n",
      "\n",
      "titulo_noticias\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 46.47%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 61.54%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 49.76%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 58.24%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 61.22%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 46.47%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#all_datasets\n",
    "svd = TruncatedSVD(n_components=100, n_iter=50, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X_svd = lsa.fit_transform(X_tfidf)\n",
    "\n",
    "print(\"\\nall_datasets\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, avalencias))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "#tweets_mg\n",
    "X_svd = lsa.fit_transform(X_tfidf_tmg)\n",
    "\n",
    "print(\"\\ntweets_mg\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, aval_tweets_mg))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "#titulo_noticias\n",
    "X_svd = lsa.fit_transform(X_tfidf_tn)\n",
    "\n",
    "print(\"\\ntitulo_noticias\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, aval_titulo_noticias))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gV9r8SSlNUd"
   },
   "source": [
    "## LDA (usando TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "id": "hYzcOnavlS6o",
    "outputId": "c3065879-7b29-48ea-ca7a-001316da7f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_datasets\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 54.36%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 50.26%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 50.78%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 52.8%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 54.1%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 51.82%\n",
      "--------------------\n",
      "\n",
      "tweets_mg\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 56.6%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 54.38%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 55.94%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 55.72%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 56.16%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 55.72%\n",
      "--------------------\n",
      "\n",
      "titulo_noticias\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 51.49%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 47.88%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 48.98%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 51.81%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 51.81%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 48.82%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#all_datasets\n",
    "lda = LatentDirichletAllocation(n_components=200, max_iter=50, random_state=0, n_jobs=5)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lda = make_pipeline(lda, normalizer)\n",
    "X_lda = lda.fit_transform(X_tfidf)\n",
    "\n",
    "print(\"\\nall_datasets\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, avalencias))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "#tweets_mg\n",
    "X_lda = lda.fit_transform(X_tfidf_tmg)\n",
    "\n",
    "print(\"\\ntweets_mg\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, aval_tweets_mg))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "#titulo_noticias\n",
    "X_lda = lda.fit_transform(X_tfidf_tn)\n",
    "\n",
    "print(\"\\ntitulo_noticias\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, aval_titulo_noticias))\n",
    "    except:\n",
    "        pass      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xyJKv6JUlhox"
   },
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "id": "bohbg-sHlgop",
    "outputId": "336629b8-a138-423d-9fd2-39e6aa3dae24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_datasets\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 64.37%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 65.02%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 41.68%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 65.15%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 65.8%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 43.37%\n",
      "--------------------\n",
      "\n",
      "tweets_mg\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 62.15%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 63.26%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 49.28%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 61.38%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 61.49%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 43.51%\n",
      "--------------------\n",
      "\n",
      "titulo_noticias\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 66.09%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 64.68%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 37.36%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 67.19%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 67.5%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 46.47%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#all_datasets\n",
    "vec_count = CountVectorizer(ngram_range=(1, 2))\n",
    "X_count = vec_count.fit_transform(afrases)\n",
    "\n",
    "print(\"\\nall_datasets\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_count, avalencias))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "      \n",
    "#tweets_mg\n",
    "vec_count = CountVectorizer(ngram_range=(1, 2))\n",
    "X_count_tmg = vec_count.fit_transform(atweets_mg)\n",
    "\n",
    "print(\"\\ntweets_mg\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_count_tmg, aval_tweets_mg))\n",
    "    except:\n",
    "        pass\n",
    "      \n",
    "      \n",
    "#titulo_noticias\n",
    "vec_count = CountVectorizer(ngram_range=(1, 2))\n",
    "X_count_tn = vec_count.fit_transform(atitulo_noticias)\n",
    "\n",
    "print(\"\\ntitulo_noticias\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_count_tn, aval_titulo_noticias))\n",
    "    except:\n",
    "        pass      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyGZBGpklwRd"
   },
   "source": [
    "## LSA (usando Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "id": "uUzc5Dr9lx9S",
    "outputId": "e40ec931-ab0c-474b-b065-2c543c487ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_datasets\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 42.39%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 59.49%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 55.66%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 59.82%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 60.79%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 46.88%\n",
      "--------------------\n",
      "\n",
      "tweets_mg\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 49.94%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 62.71%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 60.16%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 63.15%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 63.6%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 58.6%\n",
      "--------------------\n",
      "\n",
      "titulos_noticias\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 46.47%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 60.6%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 46.15%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 57.77%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 59.81%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 46.47%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#all_datasets\n",
    "svd = TruncatedSVD(n_components=100, n_iter=50, random_state=0)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lda = make_pipeline(svd, normalizer)\n",
    "X_svd = lda.fit_transform(X_count)\n",
    "\n",
    "print(\"\\nall_datasets\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, avalencias))\n",
    "    except:\n",
    "        pass\n",
    "      \n",
    "\n",
    "#tweets_mg\n",
    "X_svd = lda.fit_transform(X_count_tmg)\n",
    "\n",
    "print(\"\\ntweets_mg\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, aval_tweets_mg))\n",
    "    except:\n",
    "        pass\n",
    "      \n",
    "      \n",
    "#titulos_noticias\n",
    "X_svd = lda.fit_transform(X_count_tn)\n",
    "\n",
    "print(\"\\ntitulos_noticias\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_svd, aval_titulo_noticias))\n",
    "    except:\n",
    "        pass      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIN0NwXTmFva"
   },
   "source": [
    "## LDA (usando Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "id": "MoMB5nY4mJwM",
    "outputId": "5eaf4044-4f8d-46a9-f699-71840fcd854d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_datasets\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 54.81%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 53.32%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 52.41%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 54.1%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 54.29%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 48.63%\n",
      "--------------------\n",
      "\n",
      "tweets_mg\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 61.71%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 59.93%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 56.71%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 57.71%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 59.49%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 57.05%\n",
      "--------------------\n",
      "\n",
      "titulo_noticias\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 49.76%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 46.31%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 43.49%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 49.14%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 48.51%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 46.47%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#all_datasets\n",
    "lda = LatentDirichletAllocation(n_components=200, max_iter=50, random_state=0, n_jobs=5)\n",
    "normalizer = MinMaxScaler(copy=False)\n",
    "lda = make_pipeline(lda, normalizer)\n",
    "X_lda = lda.fit_transform(X_count)\n",
    "\n",
    "print(\"\\nall_datasets\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, avalencias))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "#tweets_mg\n",
    "X_lda = lda.fit_transform(X_count_tmg)\n",
    "\n",
    "print(\"\\ntweets_mg\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, aval_tweets_mg))\n",
    "    except:\n",
    "        pass\n",
    "      \n",
    "      \n",
    "#titulo_noticias\n",
    "X_lda = lda.fit_transform(X_count_tn)\n",
    "\n",
    "print(\"\\ntitulo_noticias\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X_lda, aval_titulo_noticias))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRkHm_VjdG36"
   },
   "source": [
    "## Count + TF-IDF + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xgvYhs55dLsM",
    "outputId": "2c9cd2ce-e399-4381-ae30-ecfca12bc0ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39902405, 45879000)"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_datasets\n",
    "# Count\n",
    "vec_count = CountVectorizer()\n",
    "X_count = vec_count.fit_transform(afrases)\n",
    "weights_count = pd.DataFrame(np.round(X_count.toarray().T, 8), index=vec_count.get_feature_names())\n",
    "\n",
    "# TF-IDF\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vec_tfidf.fit_transform(afrases)\n",
    "weights_tfidf = pd.DataFrame(np.round(X_tfidf.toarray().T, 8), index=vec_tfidf.get_feature_names())\n",
    "\n",
    "# Word2Vec preprocessing\n",
    "frases_w2v = []\n",
    "for frase in afrases:\n",
    "    bigram = []\n",
    "    p_frase = word_tokenize(frase)\n",
    "    for m, palavra in enumerate(p_frase):\n",
    "        next_p = None\n",
    "        try:\n",
    "            next_p = p_frase[m+1]\n",
    "        except:\n",
    "            pass\n",
    "        bigram += [f'{palavra}']\n",
    "#         if next_p:\n",
    "#             bigram += [f'{palavra} {next_p}']\n",
    "    frases_w2v += [bigram]\n",
    "\n",
    "# Word2Vec\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=frases_w2v,\n",
    "    sg=1,\n",
    "    hs=1,\n",
    "    size=1,\n",
    "    window=25,\n",
    "    min_count=1,\n",
    "    seed=0,\n",
    "    workers=10)\n",
    "model.train(frases_w2v, total_examples=len(frases_w2v), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "07BlXxa5h1_c",
    "outputId": "061a7282-3c97-4685-dbb1-9aa8162bc34f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all_datasets\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 63.2%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 60.4%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 62.35%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 65.08%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 52.47%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#all_datasets\n",
    "r_words = {}\n",
    "for word in vec_count.get_feature_names():\n",
    "    idx = weights_count.index.get_loc(word)\n",
    "    w2c_val = .1\n",
    "    try:\n",
    "        w2c_val = model.wv[word]\n",
    "    except:\n",
    "        pass\n",
    "    r_words[word] = (weights_tfidf.iloc[idx].values + weights_count.iloc[idx].values) * w2c_val\n",
    "lwor = list(r_words.keys())\n",
    "X = np.asarray(list(r_words.values()))\n",
    "weights = pd.DataFrame(X, index=lwor)\n",
    "X = X.T\n",
    "\n",
    "normalizer = Normalizer(copy=False)\n",
    "X = normalizer.fit_transform(X)\n",
    "\n",
    "print(\"\\nall_datasets\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X, avalencias))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wPAWw7qtgY3g",
    "outputId": "decaf607-0583-4d80-dad1-e71abac53af4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23296133, 29024000)"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets_mg\n",
    "# Count\n",
    "vec_count = CountVectorizer()\n",
    "X_count = vec_count.fit_transform(atweets_mg)\n",
    "weights_count = pd.DataFrame(np.round(X_count.toarray().T, 8), index=vec_count.get_feature_names())\n",
    "\n",
    "# TF-IDF\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vec_tfidf.fit_transform(atweets_mg)\n",
    "weights_tfidf = pd.DataFrame(np.round(X_tfidf.toarray().T, 8), index=vec_tfidf.get_feature_names())\n",
    "\n",
    "# Word2Vec preprocessing\n",
    "frases_w2v = []\n",
    "for frase in atweets_mg:\n",
    "    bigram = []\n",
    "    p_frase = word_tokenize(frase)\n",
    "    for m, palavra in enumerate(p_frase):\n",
    "        next_p = None\n",
    "        try:\n",
    "            next_p = p_frase[m+1]\n",
    "        except:\n",
    "            pass\n",
    "        bigram += [f'{palavra}']\n",
    "#         if next_p:\n",
    "#             bigram += [f'{palavra} {next_p}']\n",
    "    frases_w2v += [bigram]\n",
    "\n",
    "# Word2Vec\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=frases_w2v,\n",
    "    sg=1,\n",
    "    hs=1,\n",
    "    size=1,\n",
    "    window=25,\n",
    "    min_count=1,\n",
    "    seed=0,\n",
    "    workers=10)\n",
    "model.train(frases_w2v, total_examples=len(frases_w2v), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "HAytLOTZiRkF",
    "outputId": "79f1600e-0c4b-4bbd-f939-b616fd138470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tweets_mg\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 64.48%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 59.6%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 61.04%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 62.38%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 43.51%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#tweets_mg\n",
    "r_words = {}\n",
    "for word in vec_count.get_feature_names():\n",
    "    idx = weights_count.index.get_loc(word)\n",
    "    w2c_val = .1\n",
    "    try:\n",
    "        w2c_val = model.wv[word]\n",
    "    except:\n",
    "        pass\n",
    "    r_words[word] = (weights_tfidf.iloc[idx].values + weights_count.iloc[idx].values) * w2c_val\n",
    "lwor = list(r_words.keys())\n",
    "X = np.asarray(list(r_words.values()))\n",
    "weights = pd.DataFrame(X, index=lwor)\n",
    "X = X.T\n",
    "\n",
    "normalizer = Normalizer(copy=False)\n",
    "X = normalizer.fit_transform(X)\n",
    "\n",
    "print(\"\\ntweets_mg\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X, aval_tweets_mg))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LGu8X9bxgZGI",
    "outputId": "5779fd2b-7993-4c12-eef9-9de872ec32f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14144745, 16855000)"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#titulo_noticias\n",
    "# Count\n",
    "vec_count = CountVectorizer()\n",
    "X_count = vec_count.fit_transform(atitulo_noticias)\n",
    "weights_count = pd.DataFrame(np.round(X_count.toarray().T, 8), index=vec_count.get_feature_names())\n",
    "\n",
    "# TF-IDF\n",
    "vec_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vec_tfidf.fit_transform(atitulo_noticias)\n",
    "weights_tfidf = pd.DataFrame(np.round(X_tfidf.toarray().T, 8), index=vec_tfidf.get_feature_names())\n",
    "\n",
    "# Word2Vec preprocessing\n",
    "frases_w2v = []\n",
    "for frase in atitulo_noticias:\n",
    "    bigram = []\n",
    "    p_frase = word_tokenize(frase)\n",
    "    for m, palavra in enumerate(p_frase):\n",
    "        next_p = None\n",
    "        try:\n",
    "            next_p = p_frase[m+1]\n",
    "        except:\n",
    "            pass\n",
    "        bigram += [f'{palavra}']\n",
    "#         if next_p:\n",
    "#             bigram += [f'{palavra} {next_p}']\n",
    "    frases_w2v += [bigram]\n",
    "\n",
    "# Word2Vec\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences=frases_w2v,\n",
    "    sg=1,\n",
    "    hs=1,\n",
    "    size=1,\n",
    "    window=25,\n",
    "    min_count=1,\n",
    "    seed=0,\n",
    "    workers=10)\n",
    "model.train(frases_w2v, total_examples=len(frases_w2v), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "vvKIhKXQif9K",
    "outputId": "76418614-74e3-4e4c-fae7-d5bff5cedc28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "titulo_noticias\n",
      "Modelo   : MultinomialNB\n",
      "Acurácia : 61.22%\n",
      "--------------------\n",
      "Modelo   : RandomForestClassifier\n",
      "Acurácia : 62.95%\n",
      "--------------------\n",
      "Modelo   : KNeighborsClassifier\n",
      "Acurácia : 59.81%\n",
      "--------------------\n",
      "Modelo   : MLPClassifier\n",
      "Acurácia : 64.36%\n",
      "--------------------\n",
      "Modelo   : LinearSVC\n",
      "Acurácia : 67.5%\n",
      "--------------------\n",
      "Modelo   : SVC\n",
      "Acurácia : 46.47%\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#titulo_noticias\n",
    "r_words = {}\n",
    "for word in vec_count.get_feature_names():\n",
    "    idx = weights_count.index.get_loc(word)\n",
    "    w2c_val = .1\n",
    "    try:\n",
    "        w2c_val = model.wjv[word]\n",
    "    except:\n",
    "        pass\n",
    "    r_words[word] = (weights_tfidf.iloc[idx].values + weights_count.iloc[idx].values) * w2c_val\n",
    "lwor = list(r_words.keys())\n",
    "X = np.asarray(list(r_words.values()))\n",
    "weights = pd.DataFrame(X, index=lwor)\n",
    "X = X.T\n",
    "\n",
    "normalizer = Normalizer(copy=False)\n",
    "X = normalizer.fit_transform(X)\n",
    "\n",
    "print(\"\\ntitulo_noticias\")\n",
    "for classifier in classifiers:\n",
    "    try:\n",
    "        run_ml_model(classifier, **split_data(X, aval_titulo_noticias))\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "testes-valencia-projeto-final-ia369y-2sem-2018.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "211px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
